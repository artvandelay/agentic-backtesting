# Reflection Pattern Implementation Guide

## 🎯 Overview

This implementation brings **Chapter 4: Reflection Pattern** from the Agentic Design Patterns textbook into NLBT, with an autonomous multi-phase workflow where the LLM controls phase transitions.

## 📖 Textbook Patterns Applied

### 1. **Reflection Pattern (Chapter 4)**
**Core Concept:** "An agent evaluates its own work, output, or internal state and uses that evaluation to improve its performance."

**Our Implementation:**
- **Producer-Critic Model**: Separate prompts for generation vs. evaluation
- **Iterative Refinement**: Each phase has internal reflection loops
- **Meta-Cognition**: LLM decides when it has sufficient info/quality to proceed

### 2. **Planning Pattern (Chapter 6)**  
**Core Concept:** "Evaluating a proposed plan and identifying potential flaws or improvements before execution."

**Our Implementation:**
- Phase 1 (Understanding) is explicit planning: gather requirements first
- Phase 3 (Reporting) plans report structure before writing
- No code generation until LLM confirms requirements are complete

### 3. **Goal Setting & Monitoring (Chapter 11)**
**Core Concept:** "A goal provides the ultimate benchmark for self-evaluation while monitoring tracks progress."

**Our Implementation:**
- Each phase has explicit success criteria
- LLM self-monitors against those criteria
- Phase transitions only when LLM confirms goal achievement

---

## 🏗️ Architecture

### Three-Phase Workflow

```
┌─────────────────────────────────────────────────────────────┐
│                   PHASE 1: UNDERSTANDING                     │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ User Message → LLM Analyzes → Extract Requirements   │   │
│  │         ↓                            ↓                │   │
│  │   Complete?  ←──NO───  Ask Clarifying Question       │   │
│  │         ↓                                             │   │
│  │       YES                                             │   │
│  │         ↓                                             │   │
│  │  Transition to Phase 2                               │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                  PHASE 2: IMPLEMENTATION                     │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  Producer: Generate Code                             │   │
│  │         ↓                                             │   │
│  │  Execute in Sandbox                                   │   │
│  │         ↓                                             │   │
│  │  Critic: Evaluate Results                            │   │
│  │         ↓                                             │   │
│  │  Meets Requirements? ←──NO── Refine & Retry          │   │
│  │         ↓                                             │   │
│  │       YES                                             │   │
│  │         ↓                                             │   │
│  │  Transition to Phase 3                               │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                    PHASE 3: REPORTING                        │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  Plan Report Structure                               │   │
│  │         ↓                                             │   │
│  │  Write Report Draft                                   │   │
│  │         ↓                                             │   │
│  │  Refine Report (Critic Reviews)                      │   │
│  │         ↓                                             │   │
│  │  Save Final Report                                    │   │
│  │         ↓                                             │   │
│  │  COMPLETE                                             │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

---

## 🔍 Phase Details

### Phase 1: Understanding (Requirements Gathering)

**Textbook Principle:** "By learning from past critiques, agents avoid repeating errors" (Chapter 4, pg 1125)

**Implementation:**
```python
def _phase_understanding(self, user_message: str) -> Dict[str, Any]:
    # LLM analyzes conversation and decides if it has enough info
    analysis_prompt = """
    Analyze what you have and what's missing.
    
    STATUS: [READY or INCOMPLETE]
    EXTRACTED_INFO: [ticker, period, capital, strategy]
    NEXT_ACTION: [ask for missing OR confirm ready]
    """
    
    if "STATUS: READY" in analysis:
        # LLM decides to transition
        self.current_phase = Phase.IMPLEMENTATION
    else:
        # LLM asks clarifying questions
        return ask_user_for_missing_info()
```

**Key Features:**
- ✅ LLM has full autonomy to decide when sufficient
- ✅ Structured output format ensures consistent parsing
- ✅ No hardcoded validation rules - LLM uses reasoning
- ✅ Maintains conversation history for context

---

### Phase 2: Implementation (Producer-Critic Loop)

**Textbook Principle:** "The Producer Agent focuses on generation; the Critic Agent evaluates with a fresh perspective" (Chapter 4, pg 1118-1121)

**Implementation:**
```python
def _phase_implementation(self, max_attempts: int = 3) -> Dict[str, Any]:
    for attempt in range(1, max_attempts + 1):
        # PRODUCER: Generate code
        code = self._generate_initial_code()  # or _refine_code()
        
        # EXECUTE: Run in sandbox
        result = self.executor.execute(code)
        
        # CRITIC: Evaluate
        critique = self._critique_implementation(code, result)
        
        # LLM DECISION: Proceed or retry?
        if "DECISION: PROCEED" in critique:
            self.current_phase = Phase.REPORTING
            return success
        elif "DECISION: RETRY" in critique:
            continue  # Critic says try again
```

**Producer Prompt:**
```
You are a Python backtesting expert. Write COMPLETE, EXECUTABLE code.
Requirements: [extracted from Phase 1]
Write ONLY the Python code, no explanations.
```

**Critic Prompt:**
```
You are a SENIOR BACKTESTING ENGINEER reviewing code.
Check: ticker, period, strategy, results quality
ASSESSMENT: [PASS or FAIL]
DECISION: [PROCEED if PASS, RETRY if FAIL]
FIXES_NEEDED: [specific changes if RETRY]
```

**Key Features:**
- ✅ Separate personas prevent cognitive bias
- ✅ Critic has different role & instructions than Producer
- ✅ Multi-iteration with memory of previous attempts
- ✅ Specific, actionable feedback for refinement

---

### Phase 3: Reporting (Plan → Write → Refine)

**Textbook Principle:** "Reflection adds meta-cognition, enabling learning from outputs" (Chapter 4, pg 1158)

**Implementation:**
```python
def _phase_reporting(self) -> Dict[str, Any]:
    # Step 1: PLAN
    plan = self._plan_report()  # LLM creates outline
    
    # Step 2: WRITE
    draft = self._write_report(plan)  # LLM writes from plan
    
    # Step 3: REFINE
    final = self._refine_report(draft)  # Critic reviews & improves
    
    # Save and complete
    save_report(final)
    self.current_phase = Phase.COMPLETE
```

**Planning Prompt:**
```
Based on backtest results, plan a comprehensive report.
Create a REPORT OUTLINE:
1. Executive Summary
2. Strategy Description  
3. Results & Metrics
4. Key Insights
5. Code Appendix
```

**Refinement Prompt:**
```
You are a FINANCIAL ANALYST reviewing a backtest report.
Evaluate: clarity, metrics, documentation, insights
Provide the REFINED VERSION (not just critique).
```

**Key Features:**
- ✅ Explicit planning before execution (Chapter 6 pattern)
- ✅ Structured workflow: plan → draft → refine
- ✅ Final reflection loop ensures quality
- ✅ Auto-saves report when complete

---

## 🎮 Usage

### New Command: `nlbt reflect`

```bash
# Start reflection mode
nlbt reflect

# With specific model
nlbt reflect --model claude-3-5-sonnet-20241022

# Example session:
💭 You: Test a moving average strategy on Apple

🔍 [UNDERSTANDING] Processing...

🤖 Agent:
STATUS: INCOMPLETE
EXTRACTED_INFO:
- Ticker: AAPL ✓
- Period: MISSING
- Capital: MISSING  
- Strategy: Moving average (needs details: periods? crossover?)

NEXT_ACTION: What time period should I test (e.g., 2024, last year)?
And what's your initial capital?

💭 You: 2024 with $10,000, use 50/200 day MA crossover

🤖 Agent:
STATUS: READY
All requirements gathered!

🔄 Moving to IMPLEMENTATION phase...

⚙️ [IMPLEMENTATION] Processing...
[Generates code, executes, critiques, succeeds]

🔄 Moving to REPORTING phase...

📊 [REPORTING] Processing...
[Plans report, writes, refines, saves]

✅ Report complete! Saved to: reports/...
```

---

## 🔑 Key Design Decisions

### 1. **LLM Autonomy Over Hardcoded Rules**

**Traditional Approach:**
```python
if ticker and period and capital and strategy:
    proceed_to_implementation()
```

**Our Reflection Approach:**
```python
# Let LLM decide with reasoning
if llm.analyze("Do I have sufficient info?") == "READY":
    proceed_to_implementation()
```

**Why:** LLM can understand nuance (e.g., "last year" is sufficient even without exact dates).

---

### 2. **Structured Output Format**

Every LLM decision uses a rigid format:
```
STATUS: [READY/INCOMPLETE/PASS/FAIL]
REASONING: [explanation]
DECISION: [PROCEED/RETRY]
```

**Why:** Makes parsing reliable while preserving LLM reasoning transparency.

---

### 3. **Producer-Critic Separation**

```python
# Different system prompts
producer_prompt = "You are a Python expert. Generate code."
critic_prompt = "You are a senior engineer. Find flaws."
```

**Why:** From textbook pg 1118 - "prevents cognitive bias of self-review."

---

### 4. **Auto-Continuation Between Phases**

```python
if result.get('should_auto_continue'):
    if phase == IMPLEMENTATION:
        return self.engine.process_user_message("")  # Auto-trigger
```

**Why:** User describes strategy once; system autonomously executes all 3 phases.

---

## 📊 Comparison: Old vs. New

| Aspect | Old (`core.py`) | New (`reflection_engine.py`) |
|--------|-----------------|------------------------------|
| **Requirements** | Hardcoded checks | LLM decides sufficiency |
| **Code Gen** | Single attempt | Producer-Critic loop (3x) |
| **Error Handling** | Dump error to chat | Structured critique & refinement |
| **Reporting** | Basic markdown | Plan → Write → Refine |
| **Phase Transitions** | Implicit | Explicit LLM-controlled |
| **Textbook Alignment** | Tool Use only | Reflection + Planning + Goal Monitoring |

---

## 🧪 Testing the Reflection Pattern

### Test Case 1: Insufficient Info
```
You: "Test Apple stock"

Expected: LLM asks for period, capital, strategy
✅ Phase stays in UNDERSTANDING
```

### Test Case 2: Complete Info
```
You: "Backtest AAPL with 50/200 MA crossover, 2024, $10K"

Expected: LLM extracts all requirements, moves to IMPLEMENTATION
✅ Phase transitions automatically
```

### Test Case 3: Code Failure
```
Producer generates code with syntax error

Expected: Critic identifies error, Producer fixes it
✅ Reflection loop iterates
```

### Test Case 4: Implementation Success
```
Code executes successfully

Expected: Critic validates it meets requirements, transitions to REPORTING
✅ Phase advances autonomously
```

---

## 🎓 Textbook Alignment Summary

| Pattern | Chapter | How We Implement It |
|---------|---------|---------------------|
| **Reflection** | 4 | Producer-Critic loops in Phases 2 & 3 |
| **Prompt Chaining** | 1 | Sequential phases with context passing |
| **Planning** | 6 | Phase 1 gathers requirements before coding; Phase 3 plans report structure |
| **Tool Use** | 5 | Sandbox executor, data fetching |
| **Goal Setting** | 11 | Each phase has explicit success criteria |
| **Memory** | 8 | Conversation history maintained across phases |

---

## 🚀 Next Steps

### Immediate Enhancements
1. **Add Memory Management** (Chapter 8)
   - Compress old conversation turns
   - Maintain state dict across sessions

2. **Improve Critique Quality**
   - Add specific rubrics for evaluation
   - Use examples of good vs. bad code

3. **Guardrails** (Chapter 18)
   - Pre-execution code validation
   - Runtime limits

### Future Extensions
1. **Routing** (Chapter 2) - Different workflows for different strategy types
2. **Parallelization** (Chapter 3) - Compare multiple strategies simultaneously
3. **RAG** (Chapter 14) - Ground strategies in historical examples

---

## 💡 Key Insights

1. **"The separation of Producer and Critic is powerful because it prevents cognitive bias"** (pg 1118)
   - We use distinct prompts for generation vs. evaluation
   - Critic approaches with "fresh perspective"

2. **"Reflection enables learning from past critiques"** (pg 1125)  
   - Each retry includes history of previous failures
   - LLM sees what didn't work before

3. **"Planning involves evaluating before execution"** (pg 1149)
   - Phase 1 completes before Phase 2 starts
   - No code generation without complete requirements

4. **"Memory provides context for evaluation"** (pg 1125)
   - Conversation history informs every LLM decision
   - State accumulates across phases

---

## 📝 Configuration

Environment variables (`.env`):
```bash
LLM_MODEL=claude-3-5-sonnet-20241022  # Recommended for reasoning
CACHE_DIR=.cache
DEFAULT_TIMEOUT=120  # Longer for reflection loops
```

---

## 🎯 Summary

This implementation transforms NLBT from a **single-prompt tool** into a **multi-phase agentic system** with:

✅ **Autonomous decision-making** - LLM controls phase transitions  
✅ **Self-evaluation** - Producer-Critic pattern prevents errors  
✅ **Iterative refinement** - Reflection loops ensure quality  
✅ **Explicit planning** - Requirements before implementation  
✅ **Professional output** - Structured report generation  

All grounded in proven design patterns from the Agentic Design Patterns textbook.

